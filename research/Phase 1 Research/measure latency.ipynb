{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TEST RUN","metadata":{}},{"cell_type":"code","source":"# pip install transformers torch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# # Load a pre-trained model and tokenizer from Hugging Face\n# model_name = \"bert-base-uncased\"  # You can replace this with your desired model\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# # Set the model to evaluation mode for inference\n# model.eval()\n\n# # If you're using a GPU (like NVIDIA P100), move the model to GPU\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n\n# # Example input data for BERT (you can adjust this based on your model type)\n# inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\").to(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T08:46:29.214686Z","iopub.execute_input":"2024-09-22T08:46:29.215355Z","iopub.status.idle":"2024-09-22T08:46:29.797986Z","shell.execute_reply.started":"2024-09-22T08:46:29.215310Z","shell.execute_reply":"2024-09-22T08:46:29.796988Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import time\n\n# with torch.no_grad():\n#     start_time = time.perf_counter()\n\n#     # Perform inference\n#     outputs = model(**inputs)\n\n#     end_time = time.perf_counter()\n\n# inference_time = end_time - start_time\n# print(f\"Inference Time: {inference_time:.6f} seconds\")\n\n# with torch.no_grad():\n#     start_time = time.perf_counter()\n\n#     # Pre-process (tokenization)\n#     inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\").to(device)\n\n#     # Inference\n#     outputs = model(**inputs)\n\n#     # Post-process (getting probabilities, for example)\n#     probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n\n#     end_time = time.perf_counter()\n\n# total_latency = end_time - start_time\n# print(f\"Total End-to-End Latency: {total_latency:.6f} seconds\")\n\n# import torch\n\n# # Define batch size and number of batches\n# batch_size = 32  # Adjust this based on your requirements\n# num_batches = 100  # Total number of batches\n# input_text = [\"This is a test sentence.\"] * batch_size\n\n# # Tokenize input text for the batch\n# inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n# # Perform throughput calculation\n# start_time = time.perf_counter()\n\n# with torch.no_grad():\n#     for _ in range(num_batches):\n#         outputs = model(**inputs)\n\n# end_time = time.perf_counter()\n\n# total_time = end_time - start_time\n# throughput = (batch_size * num_batches) / total_time\n# print(f\"Throughput: {throughput:.2f} inferences/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:46:40.613647Z","iopub.execute_input":"2024-09-22T08:46:40.614036Z","iopub.status.idle":"2024-09-22T08:46:40.630952Z","shell.execute_reply.started":"2024-09-22T08:46:40.614000Z","shell.execute_reply":"2024-09-22T08:46:40.630035Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Inference Time: 0.011645 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## LOADING: LIBRARIES AND DATASETS","metadata":{}},{"cell_type":"code","source":"!pip install torch transformers datasets","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:42:34.594639Z","iopub.execute_input":"2024-09-22T14:42:34.595375Z","iopub.status.idle":"2024-09-22T14:42:48.636153Z","shell.execute_reply.started":"2024-09-22T14:42:34.595333Z","shell.execute_reply":"2024-09-22T14:42:48.635002Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport time\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:42:48.638155Z","iopub.execute_input":"2024-09-22T14:42:48.638505Z","iopub.status.idle":"2024-09-22T14:42:53.808250Z","shell.execute_reply.started":"2024-09-22T14:42:48.638465Z","shell.execute_reply":"2024-09-22T14:42:53.807470Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load dataset (SuperGLUE example)\ndataset_name_SUPER_GLUE = \"super_glue\"  # Or use \"wikitext\" for language modeling\ntask_name_SUPER_GLUE = \"rte\"  # Replace with the task name (e.g., RTE for Recognizing Textual Entailment)\n\ndataset_SUPER_GLUE = load_dataset(dataset_name_SUPER_GLUE, task_name_SUPER_GLUE, trust_remote_code=True)\nprint(\"dataset_SUPER_GLUE: \", dataset_SUPER_GLUE)\ntest_data_SUPER_GLUE = dataset_SUPER_GLUE['test']  # Use test split for evaluation\n\ndataset_name_WIKITEXT = \"wikitext\"  # Or use \"wikitext\" for language modeling\ntask_name_WIKITEXT = \"wikitext-2-v1\"  # Replace with the task name (e.g., RTE for Recognizing Textual Entailment)\n\ndataset_WIKITEXT = load_dataset(dataset_name_WIKITEXT, task_name_WIKITEXT, trust_remote_code=True)\nprint(\"dataset_WIKITEXT: \", dataset_WIKITEXT)\ntest_data_WIKITEXT = dataset_WIKITEXT['test']  # Use test split for evaluation","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:42:53.809697Z","iopub.execute_input":"2024-09-22T14:42:53.810203Z","iopub.status.idle":"2024-09-22T14:43:03.185933Z","shell.execute_reply.started":"2024-09-22T14:42:53.810168Z","shell.execute_reply":"2024-09-22T14:43:03.184910Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/30.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78297316cb4b483291e01f81678084e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/18.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96ac66e22254230aa527845c668434e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/751k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"badc8be8e601481f9a8bafd47c798c0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69bbd16e51f4db2a2a64ab34100b5b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2126df8da5cc441f835fd4423c1e4d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084dc0d209724182a727f3b1337f1ad3"}},"metadata":{}},{"name":"stdout","text":"dataset_SUPER_GLUE:  DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 2490\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 277\n    })\n    test: Dataset({\n        features: ['premise', 'hypothesis', 'idx', 'label'],\n        num_rows: 3000\n    })\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48cb535e9adb444e88044fdb4015be61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/685k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"659d992997494258bb374b463abcbf75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1f666ec4f94d33b9d720e95eb6ee3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/618k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a9092f0cf884c7db6984a39c12b83b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8d9e303ef14cdeaa7cc70ee9358879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556e2667ecb94736943ea02bead0b9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef0699faad041c1968d6fdd0db18f93"}},"metadata":{}},{"name":"stdout","text":"dataset_WIKITEXT:  DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## HUGGINGFACE LOGIN ","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:43:03.187794Z","iopub.execute_input":"2024-09-22T14:43:03.188136Z","iopub.status.idle":"2024-09-22T14:43:03.309535Z","shell.execute_reply.started":"2024-09-22T14:43:03.188099Z","shell.execute_reply":"2024-09-22T14:43:03.308420Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## bert-base-uncased","metadata":{}},{"cell_type":"code","source":"# Select your model and tokenizer\nmodel_name = \"bert-base-uncased\"  # Replace with the model you're testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load the appropriate model (use causal LM for GPT-type models)\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:53:08.108511Z","iopub.execute_input":"2024-09-22T08:53:08.109288Z","iopub.status.idle":"2024-09-22T08:53:08.682063Z","shell.execute_reply.started":"2024-09-22T08:53:08.109248Z","shell.execute_reply":"2024-09-22T08:53:08.681168Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n# print(\"total_inference_time: \", total_inference_time)\n# print(\"total_end_to_end_time: \", total_end_to_end_time)\n# print(\"num_batches: \", num_batches)\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n# print(\"total_inference_time: \", total_inference_time)\n# print(\"total_end_to_end_time: \", total_end_to_end_time)\n# print(\"num_batches: \", num_batches)\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:15:30.629638Z","iopub.execute_input":"2024-09-22T09:15:30.630309Z","iopub.status.idle":"2024-09-22T09:15:55.252828Z","shell.execute_reply.started":"2024-09-22T09:15:30.630269Z","shell.execute_reply":"2024-09-22T09:15:55.251849Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  93\n--------------WIKITEXT-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  136\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:15:57.173091Z","iopub.execute_input":"2024-09-22T09:15:57.173487Z","iopub.status.idle":"2024-09-22T09:15:57.180648Z","shell.execute_reply.started":"2024-09-22T09:15:57.173449Z","shell.execute_reply":"2024-09-22T09:15:57.179686Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 0.470590 seconds\nTotal End-to-End Latency: 4.298497 seconds\nThroughput: 6374.97 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 0.766249 seconds\nTotal End-to-End Latency: 20.254052 seconds\nThroughput: 5687.44 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## DistilBERT","metadata":{}},{"cell_type":"code","source":"# Select your model and tokenizer\nmodel_name = \"distilbert/distilbert-base-uncased\"  # Replace with the model you're testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load the appropriate model (use causal LM for GPT-type models)\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:34:50.395436Z","iopub.execute_input":"2024-09-22T09:34:50.396182Z","iopub.status.idle":"2024-09-22T09:34:56.213847Z","shell.execute_reply.started":"2024-09-22T09:34:50.396139Z","shell.execute_reply":"2024-09-22T09:34:56.212894Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf3d6f9a64240efba9bff1d23358155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b1cedf8970462b8fc9a23340fbe0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfd44fbec2c446e9d5b23ea5d391a0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10556374d3840dd8f697353b8f94f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d53e91666194ed8b3b73aa054d771ec"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:35:00.709711Z","iopub.execute_input":"2024-09-22T09:35:00.710134Z","iopub.status.idle":"2024-09-22T09:35:25.665084Z","shell.execute_reply.started":"2024-09-22T09:35:00.710096Z","shell.execute_reply":"2024-09-22T09:35:25.664117Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\n--------------WIKITEXT-------------------\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:35:28.238200Z","iopub.execute_input":"2024-09-22T09:35:28.238591Z","iopub.status.idle":"2024-09-22T09:35:28.245411Z","shell.execute_reply.started":"2024-09-22T09:35:28.238554Z","shell.execute_reply":"2024-09-22T09:35:28.244488Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 0.606713 seconds\nTotal End-to-End Latency: 4.178262 seconds\nThroughput: 4944.67 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 0.833289 seconds\nTotal End-to-End Latency: 20.703701 seconds\nThroughput: 5229.88 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## DistilGPT2","metadata":{}},{"cell_type":"code","source":"# # Load a pre-trained model and tokenizer from Hugging Face\n# model_name = \"distilbert/distilgpt2\"  # You can replace this with your desired model\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\n# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n# model.config.pad_token_id = model.config.eos_token_id\n\n# # Set the model to evaluation mode for inference\n# model.eval()\n\n# # If you're using a GPU (like NVIDIA P100), move the model to GPU\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n\n\n# Select your model and tokenizer\nmodel_name = \"distilbert/distilgpt2\"  # Replace with the model you're testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Load the appropriate model (use causal LM for GPT-type models)\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:27:24.407827Z","iopub.execute_input":"2024-09-22T09:27:24.408502Z","iopub.status.idle":"2024-09-22T09:27:24.974596Z","shell.execute_reply.started":"2024-09-22T09:27:24.408461Z","shell.execute_reply":"2024-09-22T09:27:24.973663Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n# print(\"total_inference_time: \", total_inference_time)\n# print(\"total_end_to_end_time: \", total_end_to_end_time)\n# print(\"num_batches: \", num_batches)\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\n# total_inference_time_WIKITEXT = total_inference_time\n# total_end_to_end_time_WIKITEXT = total_end_to_end_time\n# num_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:27:27.184749Z","iopub.execute_input":"2024-09-22T09:27:27.185607Z","iopub.status.idle":"2024-09-22T09:28:06.753249Z","shell.execute_reply.started":"2024-09-22T09:27:27.185566Z","shell.execute_reply":"2024-09-22T09:28:06.752309Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  93\n--------------WIKITEXT-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  136\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:28:08.767533Z","iopub.execute_input":"2024-09-22T09:28:08.767944Z","iopub.status.idle":"2024-09-22T09:28:08.774519Z","shell.execute_reply.started":"2024-09-22T09:28:08.767905Z","shell.execute_reply":"2024-09-22T09:28:08.773572Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 0.772351 seconds\nTotal End-to-End Latency: 6.969555 seconds\nThroughput: 3884.24 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 2.710015 seconds\nTotal End-to-End Latency: 32.524253 seconds\nThroughput: 1608.11 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## GPT2","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained model and tokenizer from Hugging Face\nmodel_name = \"openai-community/gpt2\"  # You can replace this with your desired model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Set the model to evaluation mode for inference\nmodel.eval()\n\n# If you're using a GPU (like NVIDIA P100), move the model to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Example input data for BERT (you can adjust this based on your model type)\n# inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:30:29.703366Z","iopub.execute_input":"2024-09-22T09:30:29.703781Z","iopub.status.idle":"2024-09-22T09:30:37.068309Z","shell.execute_reply.started":"2024-09-22T09:30:29.703731Z","shell.execute_reply":"2024-09-22T09:30:37.067362Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2667f8cc5f44600808570e522c398ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf4f3b1501e40a59cf140493510d936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d61f235a3ac4b71b9635cfa197f85fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369b664f622c485abfe8ddc157d4917d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf227023214b48cb8de4e5ed71c19d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e474ae349e074b0981c9fe5e354b059c"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:30:41.803992Z","iopub.execute_input":"2024-09-22T09:30:41.804638Z","iopub.status.idle":"2024-09-22T09:31:30.688813Z","shell.execute_reply.started":"2024-09-22T09:30:41.804598Z","shell.execute_reply":"2024-09-22T09:31:30.687850Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  93\n--------------WIKITEXT-------------------\ntotal_inference_time:  0.0\ntotal_end_to_end_time:  0.0\nnum_batches:  136\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:31:39.053593Z","iopub.execute_input":"2024-09-22T09:31:39.054508Z","iopub.status.idle":"2024-09-22T09:31:39.061122Z","shell.execute_reply.started":"2024-09-22T09:31:39.054460Z","shell.execute_reply":"2024-09-22T09:31:39.060095Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 0.860622 seconds\nTotal End-to-End Latency: 8.509414 seconds\nThroughput: 3485.85 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 1.326992 seconds\nTotal End-to-End Latency: 40.298510 seconds\nThroughput: 3284.12 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## FLAN T5 Base","metadata":{}},{"cell_type":"code","source":"# Select your model and tokenizer\nmodel_name = \"google/flan-t5-base\"  # Replace with the model you're testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Load the appropriate model (use causal LM for GPT-type models)\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:33:52.423549Z","iopub.execute_input":"2024-09-22T13:33:52.424220Z","iopub.status.idle":"2024-09-22T13:33:53.852573Z","shell.execute_reply.started":"2024-09-22T13:33:52.424181Z","shell.execute_reply":"2024-09-22T13:33:53.851667Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"T5ForSequenceClassification(\n  (transformer): T5Model(\n    (shared): Embedding(32128, 768)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classification_head): T5ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.0, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Adjust this section for Seq2Seq models like T5\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # For T5/Flan models\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n\n# def run_inference_seq2seq(data_batch):\n#     # Tokenize the input batch for Seq2Seq models\n#     inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n#     # Measure inference time\n#     with torch.no_grad():\n#         start_time = time.perf_counter()\n#         # Use the generate method for Seq2Seq models\n#         outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n#         end_time = time.perf_counter()\n\n#     # Calculate inference time\n#     inference_time = end_time - start_time\n#     return inference_time, outputs\n\n# # For classification models, continue using the original `run_inference` method\n# def run_inference_classification(data_batch):\n#     inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n#     with torch.no_grad():\n#         start_time = time.perf_counter()\n#         outputs = model(**inputs)\n#         end_time = time.perf_counter()\n\n#     inference_time = end_time - start_time\n#     return inference_time, outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:44:27.584439Z","iopub.execute_input":"2024-09-22T13:44:27.585191Z","iopub.status.idle":"2024-09-22T13:44:29.088446Z","shell.execute_reply.started":"2024-09-22T13:44:27.585148Z","shell.execute_reply":"2024-09-22T13:44:29.087428Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")\n\n# # Example: Adjust the inference function based on the model type\n# if \"gpt\" in model_name or \"llama\" in model_name:\n#     inference_fn = run_inference_classification\n# else:\n#     inference_fn = run_inference_seq2seq\n\n# # Loop through the test dataset and measure latency metrics (for both SuperGLUE and Wikitext)\n# for i in range(0, len(test_data_SUPER_GLUE), batch_size):\n#     batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']\n#     start_time = time.perf_counter()\n#     inference_time, _ = inference_fn(batch)  # Use appropriate function\n#     total_inference_time += inference_time\n#     end_time = time.perf_counter()\n#     end_to_end_latency = end_time - start_time\n#     total_end_to_end_time += end_to_end_latency\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:44:32.402566Z","iopub.execute_input":"2024-09-22T13:44:32.402962Z","iopub.status.idle":"2024-09-22T13:55:36.041577Z","shell.execute_reply.started":"2024-09-22T13:44:32.402925Z","shell.execute_reply":"2024-09-22T13:55:36.040616Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\n--------------WIKITEXT-------------------\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\n# print(total_end_to_end_time)\n# print(total_inference_time / len(test_data_SUPER_GLUE))\n# print(len(test_data_SUPER_GLUE) / total_inference_time)\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:55:46.979823Z","iopub.execute_input":"2024-09-22T13:55:46.980171Z","iopub.status.idle":"2024-09-22T13:55:46.987328Z","shell.execute_reply.started":"2024-09-22T13:55:46.980139Z","shell.execute_reply":"2024-09-22T13:55:46.986422Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 178.467590 seconds\nTotal End-to-End Latency: 179.049172 seconds\nThroughput: 16.81 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 482.830195 seconds\nTotal End-to-End Latency: 484.487496 seconds\nThroughput: 9.03 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## google/t5-efficient-tiny","metadata":{}},{"cell_type":"code","source":"# Adjust this section for Seq2Seq models like T5\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # For T5/Flan models\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n\n# def run_inference_seq2seq(data_batch):\n#     # Tokenize the input batch for Seq2Seq models\n#     inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n#     # Measure inference time\n#     with torch.no_grad():\n#         start_time = time.perf_counter()\n#         # Use the generate method for Seq2Seq models\n#         outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n#         end_time = time.perf_counter()\n\n#     # Calculate inference time\n#     inference_time = end_time - start_time\n#     return inference_time, outputs\n\n# # For classification models, continue using the original `run_inference` method\n# def run_inference_classification(data_batch):\n#     inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n#     with torch.no_grad():\n#         start_time = time.perf_counter()\n#         outputs = model(**inputs)\n#         end_time = time.perf_counter()\n\n#     inference_time = end_time - start_time\n#     return inference_time, outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:00:39.835997Z","iopub.execute_input":"2024-09-22T14:00:39.836682Z","iopub.status.idle":"2024-09-22T14:00:41.536726Z","shell.execute_reply.started":"2024-09-22T14:00:39.836640Z","shell.execute_reply":"2024-09-22T14:00:41.535767Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")\n\n# # Example: Adjust the inference function based on the model type\n# if \"gpt\" in model_name or \"llama\" in model_name:\n#     inference_fn = run_inference_classification\n# else:\n#     inference_fn = run_inference_seq2seq\n\n# # Loop through the test dataset and measure latency metrics (for both SuperGLUE and Wikitext)\n# for i in range(0, len(test_data_SUPER_GLUE), batch_size):\n#     batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']\n#     start_time = time.perf_counter()\n#     inference_time, _ = inference_fn(batch)  # Use appropriate function\n#     total_inference_time += inference_time\n#     end_time = time.perf_counter()\n#     end_to_end_latency = end_time - start_time\n#     total_end_to_end_time += end_to_end_latency\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:00:45.806177Z","iopub.execute_input":"2024-09-22T14:00:45.806579Z","iopub.status.idle":"2024-09-22T14:11:51.370145Z","shell.execute_reply.started":"2024-09-22T14:00:45.806534Z","shell.execute_reply":"2024-09-22T14:11:51.369108Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"--------------SUPERGLUE-------------------\n--------------WIKITEXT-------------------\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\n# print(total_end_to_end_time)\n# print(total_inference_time / len(test_data_SUPER_GLUE))\n# print(len(test_data_SUPER_GLUE) / total_inference_time)\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:11:51.371636Z","iopub.execute_input":"2024-09-22T14:11:51.371952Z","iopub.status.idle":"2024-09-22T14:11:51.378770Z","shell.execute_reply.started":"2024-09-22T14:11:51.371919Z","shell.execute_reply":"2024-09-22T14:11:51.377804Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 179.477918 seconds\nTotal End-to-End Latency: 180.073888 seconds\nThroughput: 16.72 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 483.695018 seconds\nTotal End-to-End Latency: 485.383648 seconds\nThroughput: 9.01 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TinyLlama v1.1","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained model and tokenizer from Hugging Face\nmodel_name = \"TinyLlama/TinyLlama_v1.1\"  # You can replace this with your desired model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Set the model to evaluation mode for inference\nmodel.eval()\n\n# If you're using a GPU (like NVIDIA P100), move the model to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Example input data for BERT (you can adjust this based on your model type)\n# inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:37:28.769465Z","iopub.execute_input":"2024-09-22T09:37:28.770230Z","iopub.status.idle":"2024-09-22T09:37:46.555999Z","shell.execute_reply.started":"2024-09-22T09:37:28.770190Z","shell.execute_reply":"2024-09-22T09:37:46.555054Z"},"trusted":true},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be951cee89364c5a8d7c6cb332a2159a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f34876830e064a2587b078260134d576"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1c434e95a844358b57fddf09ad8438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8518ee8f7077454791750c540d8c6c55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd72ff6eaf24b6e93f181d2bef391b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56284e1d6204a08be461866028414f0"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama_v1.1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"LlamaForSequenceClassification(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (score): Linear(in_features=2048, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:37:51.794890Z","iopub.execute_input":"2024-09-22T09:37:51.795256Z","iopub.status.idle":"2024-09-22T09:45:44.952512Z","shell.execute_reply.started":"2024-09-22T09:37:51.795214Z","shell.execute_reply":"2024-09-22T09:45:44.951482Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"--------------SUPERGLUE-------------------\n--------------WIKITEXT-------------------\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T09:48:36.559049Z","iopub.execute_input":"2024-09-22T09:48:36.559468Z","iopub.status.idle":"2024-09-22T09:48:36.566511Z","shell.execute_reply.started":"2024-09-22T09:48:36.559428Z","shell.execute_reply":"2024-09-22T09:48:36.565551Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 2.432205 seconds\nTotal End-to-End Latency: 88.508700 seconds\nThroughput: 1233.45 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 3.343920 seconds\nTotal End-to-End Latency: 384.567217 seconds\nThroughput: 1303.26 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Gemma 2b","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained model and tokenizer from Hugging Face\nmodel_name = \"google/gemma-2-2b\"  # You can replace this with your desired model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n# model.config.pad_token_id = model.config.eos_token_id\n\n# Set the model to evaluation mode for inference\nmodel.eval()\n\n# If you're using a GPU (like NVIDIA P100), move the model to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Example input data for BERT (you can adjust this based on your model type)\n# inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:43:09.369425Z","iopub.execute_input":"2024-09-22T14:43:09.370303Z","iopub.status.idle":"2024-09-22T14:44:16.892604Z","shell.execute_reply.started":"2024-09-22T14:43:09.370261Z","shell.execute_reply":"2024-09-22T14:44:16.891631Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf2fb78c8c643f4821e8f43132bd60b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233c259b614e4fa5a598a66080d9a89a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8f0566a2b442bcb505b5399cb4ee44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e50910f59499496d9ac40387080615e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6287f953923946708606532a719b9b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38ff241f31704b86b851519bf08040b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92180af0f60d4e05959ff9428a8f00e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7abc55fc0040b0b359be8631812d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd158c952a14fe9bc8380a073af9768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"478dbb063c8b477395a35b33b08e3fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa030f5ae4b14a42bb2b8882d97bb34f"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Gemma2ForSequenceClassification(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2SdpaAttention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n  )\n  (score): Linear(in_features=2304, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:44:16.894325Z","iopub.execute_input":"2024-09-22T14:44:16.894827Z","iopub.status.idle":"2024-09-22T14:57:58.473139Z","shell.execute_reply.started":"2024-09-22T14:44:16.894790Z","shell.execute_reply":"2024-09-22T14:57:58.472174Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"--------------SUPERGLUE-------------------\n--------------WIKITEXT-------------------\n\n\nFINISHED INFERENCING!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:10:04.021701Z","iopub.execute_input":"2024-09-22T15:10:04.022145Z","iopub.status.idle":"2024-09-22T15:10:04.029094Z","shell.execute_reply.started":"2024-09-22T15:10:04.022109Z","shell.execute_reply":"2024-09-22T15:10:04.028135Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"----------------------SUPER_GLUE--------------------\nTotal Inference Time: 57.644016 seconds\nTotal End-to-End Latency: 156.882229 seconds\nThroughput: 52.04 samples/second\n----------------------WIKITEXT--------------------\nTotal Inference Time: 235.391225 seconds\nTotal End-to-End Latency: 664.604226 seconds\nThroughput: 18.51 samples/second\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Llama 2 7B","metadata":{}},{"cell_type":"code","source":"# Select your model and tokenizer\nmodel_name = \"meta-llama/Llama-2-7b-hf\"  # Replace with the model you're testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the appropriate model (use causal LM for GPT-type models)\nif \"gpt\" in model_name or \"llama\" in model_name:\n    model = AutoModelForCausalLM.from_pretrained(model_name)\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Move the model to GPU (if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport os\n\n# Set the environment variable to use all available GPUs\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Assuming the T4 GPUs are device 0 and 1\n\n# Load a pre-trained model and tokenizer from Hugging Face\nmodel_name = \"meta-llama/Llama-2-7b-hf\"  # You can replace this with your desired model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the model with distributed setup\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",  # This will automatically distribute the model across available GPUs\n    torch_dtype=torch.float16,  # Use half-precision to reduce memory usage\n)\n\nmodel.config.pad_token_id = model.config.eos_token_id\n\n# Set the model to evaluation mode for inference\nmodel.eval()\n\n# Example input data\ninput_text = \"This is a test sentence.\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Move inputs to the same device as the first parameter of the model\ndevice = next(model.parameters()).device\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate output\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=50)\n\n# Decode the output\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T10:17:31.887237Z","iopub.execute_input":"2024-09-22T10:17:31.887634Z","iopub.status.idle":"2024-09-22T10:20:02.104435Z","shell.execute_reply.started":"2024-09-22T10:17:31.887593Z","shell.execute_reply":"2024-09-22T10:20:02.103357Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d469891aef2a47de8fcd8ff368bf70fb"}},"metadata":{}},{"name":"stdout","text":"This is a test sentence.\nThe test sentence is a test sentence.\nThis is the second sentence.\nThe second sentence is the second sentence.\nThis is the third sentence.\nThe third sentence is the third sentence.\nThis\n","output_type":"stream"}]},{"cell_type":"code","source":"def run_inference(data_batch):\n    # Tokenize the input batch\n    inputs = tokenizer(data_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Measure inference time\n    with torch.no_grad():\n        start_time = time.perf_counter()\n        outputs = model(**inputs)\n        end_time = time.perf_counter()\n    \n    # Calculate inference time\n    inference_time = end_time - start_time\n    return inference_time, outputs\n\n\n# Set batch size for throughput measurement\nbatch_size = 32\n\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_SUPER_GLUE) // batch_size\nprint(\"--------------SUPERGLUE-------------------\")\n\n## SUPERGLUE\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_SUPER_GLUE), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_SUPER_GLUE[i:i + batch_size]['premise']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n    \n# print(\"--------------SUPERGLUE-------------------\")\ntotal_inference_time_SUPER_GLUE = total_inference_time\ntotal_end_to_end_time_SUPER_GLUE = total_end_to_end_time\nnum_batches_SUPER_GLUE = num_batches\n\n## WIKITEXT\n# Initialize variables to track metrics\ntotal_inference_time = 0.0\ntotal_end_to_end_time = 0.0\nnum_batches = len(test_data_WIKITEXT) // batch_size\n\nprint(\"--------------WIKITEXT-------------------\")\n\n\n# Loop through the test dataset and measure latency metrics\nfor i in range(0, len(test_data_WIKITEXT), batch_size):\n    # Get the batch (adjust based on dataset)\n    batch = test_data_WIKITEXT[i:i + batch_size]['text']  # Use 'text' for Wikitext or 'premise' for SuperGLUE RTE task\n\n    # Measure start time for end-to-end latency\n    start_time = time.perf_counter()\n\n    # Run inference and get inference time\n    inference_time, _ = run_inference(batch)\n    total_inference_time += inference_time\n\n    # Measure end-to-end latency (includes tokenization, inference, etc.)\n    end_time = time.perf_counter()\n    end_to_end_latency = end_time - start_time\n    total_end_to_end_time += end_to_end_latency\n\n# print(\"--------------WIKITEXT-------------------\")\ntotal_inference_time_WIKITEXT = total_inference_time\ntotal_end_to_end_time_WIKITEXT = total_end_to_end_time\nnum_batches_WIKITEXT = num_batches\n\nprint(\"\\n\\nFINISHED INFERENCING!!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate throughput: total samples / total inference time\n\nprint(\"----------------------SUPER_GLUE--------------------\")\nthroughput = len(test_data_SUPER_GLUE) / total_inference_time_SUPER_GLUE\nprint(f\"Total Inference Time: {total_inference_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_SUPER_GLUE:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n\nprint(\"----------------------WIKITEXT--------------------\")\nthroughput = len(test_data_WIKITEXT) / total_inference_time_WIKITEXT\nprint(f\"Total Inference Time: {total_inference_time_WIKITEXT:.6f} seconds\")\nprint(f\"Total End-to-End Latency: {total_end_to_end_time_WIKITEXT:.6f} seconds\")\nprint(f\"Throughput: {throughput:.2f} samples/second\")\n","metadata":{},"execution_count":null,"outputs":[]}]}